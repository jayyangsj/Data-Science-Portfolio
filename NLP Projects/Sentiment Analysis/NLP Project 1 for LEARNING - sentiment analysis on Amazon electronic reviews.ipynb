{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Project #1 for LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\n",
    "\n",
    "Text preprocessing includes both Stemming as well as Lemmatization. Many times people find these two terms confusing. Some treat these two as same. Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "# quick examples\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "stopwords = set(w.rstrip() for w in open('D:\\Data\\stopwords.txt'))\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "\n",
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the reviews\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open(r'D:\\Data\\sorted_data_acl\\electronics\\positive.review').read(), features=\"html5lib\")\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "negative_reviews = BeautifulSoup(open(r'D:\\Data\\sorted_data_acl\\electronics\\negative.review').read(), features=\"html5lib\")\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(positive_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is a common task in Natural Language Processing (NLP). It’s a fundamental step in both traditional NLP methods like Count Vectorizer and Advanced Deep Learning-based architectures like Transformers.\n",
    "\n",
    "Tokens are the building blocks of Natural Language.\n",
    "\n",
    "Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<review_text>\n",
       "I purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power.\n",
       "\n",
       "I feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply.\n",
       "\n",
       "As always, Amazon had it to me in &lt;2 business days\n",
       "</review_text>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at a review\n",
    "t = positive_reviews[0]\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize vs split\n",
    "\n",
    "tokenize() ,which returns a list, will ignore empty string (when a delimiter appears twice in succession) where as split() keeps such string. The split() can take regex as delimiter where as tokenize does not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'purchased', 'this', 'unit', 'due', 'to', 'frequent', 'blackouts', 'in', 'my', 'area', 'and', '2', 'power', 'supplies', 'going', 'bad', '.', 'It', 'will', 'run', 'my', 'cable', 'modem', ',', 'router', ',', 'PC', ',', 'and', 'LCD', 'monitor', 'for', '5', 'minutes', '.', 'This', 'is', 'more', 'than', 'enough', 'time', 'to', 'save', 'work', 'and', 'shut', 'down', '.', 'Equally', 'important', ',', 'I', 'know', 'that', 'my', 'electronics', 'are', 'receiving', 'clean', 'power', '.', 'I', 'feel', 'that', 'this', 'investment', 'is', 'minor', 'compared', 'to', 'the', 'loss', 'of', 'valuable', 'data', 'or', 'the', 'failure', 'of', 'equipment', 'due', 'to', 'a', 'power', 'spike', 'or', 'an', 'irregular', 'power', 'supply', '.', 'As', 'always', ',', 'Amazon', 'had', 'it', 'to', 'me', 'in', '<', '2', 'business', 'days']\n"
     ]
    }
   ],
   "source": [
    "# first let's just try to tokenize the text using nltk's tokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "print(nltk.tokenize.word_tokenize(t.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice how it doesn't downcase, so It != it\n",
    "# not only that, but do we really want to include the word \"it\" anyway?\n",
    "# you can imagine it wouldn't be any more common in a positive review than a negative review\n",
    "# so it might only add noise to our model.\n",
    "# so let's create a function that does all this pre-processing for us\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    tokens = nltk.tokenize.word_tokenize(s.lower()) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word-to-index map so that we can create our word-frequency vectors later\n",
    "# let's also save the tokenized versions so we don't have to tokenize again later\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "original_reviews = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def word_index_map_filler (review_ResultSet, current_index): \n",
    "    # word counter\n",
    "    # positive_review is a bs4.element.ResultSet\n",
    "    for review in review_ResultSet:\n",
    "        original_reviews.append(review.text)\n",
    "        tokens = my_tokenizer(review.text)\n",
    "        positive_tokenized.append(tokens)\n",
    "        for token in tokens:\n",
    "            if token not in word_index_map:\n",
    "                word_index_map[token] = current_index\n",
    "                current_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_index_map_filler(positive_reviews, current_index)\n",
    "word_index_map_filler(negative_reviews, current_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in positive_reviews:\n",
    "    original_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "for review in negative_reviews:\n",
    "    original_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_index_map): 11082\n"
     ]
    }
   ],
   "source": [
    "print(\"len(word_index_map):\", len(word_index_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create our input matrices\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # normalize it before setting label\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "# (N x D+1 matrix - keeping them together for now so we can shuffle more easily later\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22166000"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ytjya\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7736842105263158\n",
      "Test accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data and create train/test splits\n",
    "# try it multiple times!\n",
    "original_reviews, data = shuffle(original_reviews, data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "# last 100 rows will be test\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit -0.6550971099533333\n",
      "bad -0.7849407210705378\n",
      "cable 0.5801306870325602\n",
      "time -0.544523090353267\n",
      "'ve 0.7957072077930096\n",
      "month -0.7533632856858961\n",
      "sound 1.0882875689921967\n",
      "lot 0.7456031131815123\n",
      "you 0.8557737579280181\n",
      "n't -2.1542038593220236\n",
      "easy 1.7743269565224105\n",
      "quality 1.456715953872191\n",
      "company -0.5554719087880162\n",
      "item -0.9129136325144119\n",
      "wa -1.6789345805691982\n",
      "perfect 0.9490018599729955\n",
      "fast 0.9173785105050907\n",
      "ha 0.6859594066279514\n",
      "price 2.609573710768579\n",
      "value 0.5616259246403541\n",
      "money -1.0452726173907554\n",
      "memory 0.930704082657109\n",
      "picture 0.5063567942732812\n",
      "buy -0.9080229682060375\n",
      "bit 0.6784065417732267\n",
      "happy 0.6215205318288972\n",
      "pretty 0.7343964402329317\n",
      "doe -1.2195353512100804\n",
      "highly 0.9679739017035716\n",
      "recommend 0.6646476175884155\n",
      "customer -0.6812308185025778\n",
      "support -0.8357215827598483\n",
      "little 0.984985535456441\n",
      "returned -0.8095779053945313\n",
      "excellent 1.4101462192217555\n",
      "love 1.055818949581985\n",
      "home 0.5234355714572507\n",
      "week -0.7396005172883073\n",
      "using 0.6538769233256019\n",
      "laptop 0.5451236140859809\n",
      "video 0.5599373800952309\n",
      "poor -0.7602982448269386\n",
      "look 0.537175990370615\n",
      "then -1.1859511094358022\n",
      "tried -0.8182973828021962\n",
      "static -0.5035701060827926\n",
      "try -0.67475032954237\n",
      "space 0.5717466482111951\n",
      "comfortable 0.6717867543379924\n",
      "photo 0.5161402261536563\n",
      "hour -0.5319734565098103\n",
      "expected 0.5539372953577393\n",
      "speaker 0.808793899318379\n",
      "warranty -0.6973391990759173\n",
      "stopped -0.55427566382996\n",
      "returning -0.5333404908156681\n",
      "paper 0.6427788044294412\n",
      "return -1.156830439357078\n",
      "waste -1.000034180498237\n",
      "refund -0.5935881283431881\n"
     ]
    }
   ],
   "source": [
    "# let's look at the weights for each word\n",
    "# try it with different threshold values!\n",
    "threshold = 0.5\n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most WRONG positive review (prob = 0.3544260199338042, pred = 0.0):\n",
      "\n",
      "I didn't buy this on Amazon but wanted to say this device is great. The only bad thing was MY laptop is old!  Can't go wrong with this one\n",
      "\n",
      "Most WRONG negative review (prob = 0.606156367265541, pred = 1.0):\n",
      "\n",
      "The Voice recorder meets all my expectations and more\n",
      "Easy to use, easy to transfer great results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check misclassified examples\n",
    "preds = model.predict(X)\n",
    "P = model.predict_proba(X)[:,1] # p(y = 1 | x)\n",
    "\n",
    "# since there are many, just print the \"most\" wrong samples\n",
    "minP_whenYis1 = 1\n",
    "maxP_whenYis0 = 0\n",
    "wrong_positive_review = None\n",
    "wrong_negative_review = None\n",
    "wrong_positive_prediction = None\n",
    "wrong_negative_prediction = None\n",
    "for i in range(N):\n",
    "    p = P[i]\n",
    "    y = Y[i]\n",
    "    if y == 1 and p < 0.5:\n",
    "        if p < minP_whenYis1:\n",
    "            wrong_positive_review = original_reviews[i]\n",
    "            wrong_positive_prediction = preds[i]\n",
    "            minP_whenYis1 = p\n",
    "    elif y == 0 and p > 0.5:\n",
    "        if p > maxP_whenYis0:\n",
    "            wrong_negative_review = original_reviews[i]\n",
    "            wrong_negative_prediction = preds[i]\n",
    "            maxP_whenYis0 = p\n",
    "\n",
    "print(\"Most WRONG positive review (prob = %s, pred = %s):\" % (minP_whenYis1, wrong_positive_prediction))\n",
    "print(wrong_positive_review)\n",
    "print(\"Most WRONG negative review (prob = %s, pred = %s):\" % (maxP_whenYis0, wrong_negative_prediction))\n",
    "print(wrong_negative_review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
